
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TranslationPipeline

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
print("üß† –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å tilmash (KZ ‚Üí RU)...")
model = AutoModelForSeq2SeqLM.from_pretrained("issai/tilmash").to(DEVICE)
tokenizer = AutoTokenizer.from_pretrained("issai/tilmash")
tilmash = TranslationPipeline(
    model=model,
    tokenizer=tokenizer,
    src_lang="kaz_Cyrl",
    tgt_lang="rus_Cyrl",
    max_length=1000,
    device=0 if torch.cuda.is_available() else -1
)

def translate_kz_to_ru(text):
    if not text.strip():
        return ""
    return tilmash(text)[0]["translation_text"]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
base_url = "https://bilim-all.kz"
category_url_template = "https://bilim-all.kz/article/list/14?page={}"

# –§–∏–ª—å—Ç—Ä—ã
religious_keywords = [
    "–∞–ª–ª–∞", "“õ“±–¥–∞–π", "–¥—ñ–Ω", "–Ω–∞–º–∞–∑", "–æ—Ä–∞–∑–∞", "–º–µ—à—ñ—Ç", "–∏–º–∞–º", "“õ“±—Ä–∞–Ω", "—Ö–∞–¥–∏—Å",
    "—Å–∞—É–∞–ø", "–∫”ô–ø—ñ—Ä", "—Ç”ô—É–±–µ", "–¥“±“ì–∞", "–ø–∞–π“ì–∞–º–±–∞—Ä", "—à–∞—Ä–∏“ì–∞—Ç", "–∏–Ω—à–∞", "–∏—Å–ª–∞–º"
]
additional_block_keywords = [
    "—Å–∞—è—Å–∞—Ç", "“Ø–∫—ñ–º–µ—Ç", "–ø–∞—Ä—Ç–∏—è", "—Å–∞–π–ª–∞—É", "–º–∏—Ç–∏–Ω–≥", "“õ–∞–º–∞—É", "—Ç–µ—Ä–≥–µ—É", "–ø–æ–ª–∏—Ü–∏—è",
    "”©–ª—ñ–º", "–∑–æ—Ä–ª—ã“õ", "“õ—ã–ª–º—ã—Å", "—Å–æ—Ç", "—Ç–µ—Ä—Ä–æ—Ä", "–±–æ—Å“õ—ã–Ω", "—Ç”©–±–µ–ª–µ—Å", "–∞—à—Ç—ã“õ",
    "–∂–µ–º“õ–æ—Ä–ª—ã“õ", "—Ç“Ø—Ä–º–µ", "“õ—É–¥–∞–ª–∞—É", "—Ä–∞–¥–∏–∫–∞–ª", "–æ–ø–ø–æ–∑–∏—Ü–∏—è", "—Ä–µ–≤–æ–ª—é—Ü–∏—è"
]
gender_keywords = [
    "”ô–π–µ–ª", "–∞–Ω–∞–ª–∞—Ä", "–∫–µ–ª—ñ–Ω", "–µ—Ä–∫–µ–∫", "–µ—Ä –∞–¥–∞–º", "–∫“Ø–π–µ—É—ñ", "–∫“Ø–π–µ—É", "–±–∞“ì—ã–Ω—É"
]

block_keywords = religious_keywords + additional_block_keywords + gender_keywords

try:
    df = pd.read_excel("bilim_articles_300_final.xlsx")
    data = df.to_dict("records")
    seen_articles = set((row["title"].lower(), row["paragraph"][:100].lower()) for row in data)
    text_id = max(row["text_id"] for row in data) + 1
    print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞—é —Å text_id={text_id}, –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫.")
except FileNotFoundError:
    data = []
    seen_articles = set()
    text_id = 1
text_id = 1
article_links = []
seen_articles = set()

print("üîç –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏...")

for page in range(1, 40):
    url = category_url_template.format(page)
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    articles = soup.select("figure figcaption h2 a")
    for a in articles:
        full_url = base_url + a['href']
        article_links.append(full_url)
        if len(article_links) >= 1000:
            break
    if len(article_links) >= 1000:
        break
    time.sleep(0.3)

print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(article_links)} —Å—Å—ã–ª–æ–∫.\n")

MAX_ARTICLES = 215

for url in article_links:
    if text_id > MAX_ARTICLES:
        break
    if text_id > MAX_ARTICLES:
        break
    try:
        res = requests.get(url)
        soup = BeautifulSoup(res.text, "html.parser")

        title_tag = soup.select_one("div.blogtext h1.heading")
        title = title_tag.get_text(strip=True) if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        if '–∫”©–∫ —Å”©–∑' in title.lower():
            print(f"‚õîÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫”©–∫ —Å”©–∑ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏): {title}")
            continue
        title_ru = translate_kz_to_ru(title)

        author_tag = soup.select_one("div.blogmetas li a[href^='/user/profile/']")
        author = author_tag.get_text(strip=True) if author_tag else "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω"

        category_tags = soup.select("div.blogmetas li i.fa-align-justify ~ a")
        subcategory = category_tags[1].get_text(strip=True) if len(category_tags) > 1 else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
        subcategory_ru = translate_kz_to_ru(subcategory)

        paragraphs = soup.select("div.blogtext p")
        clean_paragraphs = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]

        if not clean_paragraphs:
            continue

        key = (title.lower(), clean_paragraphs[0][:100].lower())
        if key in seen_articles:
            print(f"‚õîÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title}")
            continue
        seen_articles.add(key)

        text_combined = " ".join(clean_paragraphs).lower()
        matched_words = [word for word in block_keywords if word in text_combined]
        if matched_words:
            print(f"‚õîÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ä–µ–ª–∏–≥–∏—è / –Ω–µ–≥–∞—Ç–∏–≤ / –≥–µ–Ω–¥–µ—Ä): {title} ‚Üí –°–ª–æ–≤–∞: {', '.join(matched_words)}")
            continue
            print(f"‚õîÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (—Ä–µ–ª–∏–≥–∏—è / –Ω–µ–≥–∞—Ç–∏–≤ / –≥–µ–Ω–¥–µ—Ä): {title}")
            continue

        print(f"üìÑ [{text_id}] {title} ‚Äî –∞–±–∑–∞—Ü–µ–≤: {len(clean_paragraphs)}")

        for paragraph in clean_paragraphs:
            if "”ô–ª–µ—É–º–µ—Ç—Ç—ñ–∫ –∂–µ–ª—ñ–ª–µ—Ä–¥–µ" not in paragraph.lower():
                paragraph_ru = translate_kz_to_ru(paragraph)
                data.append({
                    "text_id": text_id,
                    "paragraph": paragraph,
                    "p_ru": paragraph_ru,
                    "title": title,
                    "title_ru": title_ru,
                    "author": author,
                    "url": url,
                    "category": "–ë–∞–ª–∞ —Ç”ô—Ä–±–∏–µ—Å—ñ",
                    "subcategory": subcategory,
                    "subcategory_ru": subcategory_ru
                })

        text_id += 1
        if text_id % 10 == 0:
            print("üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
            df = pd.DataFrame(data)
            df.to_excel("bilim_articles_300_final.xlsx", index=False)
        time.sleep(0.2)

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç–∞—Ç—å–µ {url}: {e}")

df = pd.DataFrame(data)
df.to_csv("bilim_articles_300_final.csv", index=False, encoding="utf-8-sig")
df.to_excel("bilim_articles_300_final.xlsx", index=False)

print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
print(f"üî∏ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü–µ–≤): {len(df)}")
print("üî∏ CSV: bilim_articles_300_final.csv")
print("üî∏ Excel: bilim_articles_300_final.xlsx")
